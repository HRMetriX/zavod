import os
import json
import time
import random
import feedparser
from datetime import datetime, timedelta
import requests
from openai import OpenAI
import traceback
from huggingface_hub import InferenceClient

# === CONFIG ===
TELEGRAM_BOT_TOKEN = os.environ["TELEGRAM_BOT_TOKEN"]
HF_TOKEN = os.environ["HF_TOKEN"]
FB_API_KEY = os.environ["FUSIONBRAIN_API_KEY"]
GIST_TOKEN = os.environ["GIST_TOKEN"]
CHANNEL = os.environ.get("TELEGRAM_CHANNEL", "@notreviews")

RSS_SOURCES = [
    "https://ria.ru/export/rss2/archive/index.xml",
    "https://tass.ru/rss/v2.xml",
    "https://lenta.ru/rss/",
]

# === GIST STATE MANAGEMENT ===
GIST_ID = "5944017a021bcea90b63cf408a0324e5"

def load_seen():
    url = f"https://api.github.com/gists/{GIST_ID}"
    headers = {"Authorization": f"token {GIST_TOKEN}"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            files = resp.json().get("files", {})
            if "seen.json" in files:
                content = files["seen.json"].get("content", "[]")
                return set(json.loads(content))
        return set()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ seen.json –∏–∑ Gist: {e}")
        return set()

def save_seen(seen_set):
    url = f"https://api.github.com/gists/{GIST_ID}"
    headers = {"Authorization": f"token {GIST_TOKEN}"}
    payload = {
        "files": {
            "seen.json": {
                "content": json.dumps(list(seen_set), ensure_ascii=False, indent=2)
            }
        }
    }
    try:
        requests.patch(url, headers=headers, json=payload, timeout=10)
        print("‚úÖ seen.json –æ–±–Ω–æ–≤–ª—ë–Ω –≤ Gist")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Gist: {e}")

# === NEWS PARSING ===
def fetch_political_news(hours=1):
    keywords = ["–ü—É—Ç–∏–Ω", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–°–æ–≤–±–µ–∑", "–ú–∏–Ω–æ–±–æ—Ä–æ–Ω—ã", "–õ–∞–≤—Ä–æ–≤", "–®–æ–π–≥—É", "–Ω–∞–∑–Ω–∞—á", "—É–∫–∞–∑", "–°–∞–Ω—á–∏–∫", "–ë—É–ª—ã–≥–∞", "–†–æ—Å—Å–∏—è", "–ø–æ–ª–∏—Ç–∏–∫", "–°–∏", "–ó–µ–ª–µ–Ω—Å–∫", "–ë–∞–π–¥–µ–Ω", "–¢—Ä–∞–º–ø"]
    fresh = []
    cutoff = datetime.now() - timedelta(hours=hours)

    for url in RSS_SOURCES:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                pub = datetime(*entry.published_parsed[:6])
                if pub < cutoff:
                    continue
                title = entry.title
                summary = entry.get("summary", "")
                if title in seen_titles:
                    continue
                if any(kw in title or kw in summary for kw in keywords):
                    fresh.append({
                        "title": title,
                        "summary": summary[:300],
                        "link": entry.link
                    })
                    seen_titles.add(title)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
    return fresh

# === LLM ===
def generate_post_with_llm(title, summary):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–∞ —á–µ—Ä–µ–∑ Hugging Face Inference Providers (Qwen2.5 via Together)"""
    PROMPT_TEMPLATE = """
–¢—ã ‚Äî –í–∏—Ç—ë–∫ –∏–∑ –≥–∞—Ä–∞–∂–∞: –º—É–∂–∏–∫ 50+, –±—ã–≤—à–∏–π –∑–∞–≤–æ–¥—á–∞–Ω–∏–Ω, —Å –ª—ë–≥–∫–æ–π –∫–æ–Ω—Ç—É–∑–∏–µ–π –ø–æ—Å–ª–µ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –∫—Ä–∞–Ω–∞. –¢—ã –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–µ—à—å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Ç–∞–∫, –±—É–¥—Ç–æ —É—Å–ª—ã—à–∞–ª –∏—Ö –æ—Ç –°–∞–Ω –°–∞–Ω—ã—á–∞ —É –ª–∞—Ä—å–∫–∞ –∏–ª–∏ —Ç—ë—Ç–∏ –õ—é–±—ã –Ω–∞ –ª–∞–≤–∫–µ. –ù–µ –Ω–∞–∑—ã–≤–∞–π –ø–æ–ª–∏—Ç–∏–∫–æ–≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ–∑–≤–∏—â–∞: –ü—É—Ç–∏–Ω = –ë–∞—Ç–µ–Ω—å–∫–∞, –õ–∞–≤—Ä–æ–≤ = –°–ª–æ–Ω—è—Ä–∞, –°–∏ = –ö–∏—Ç–∞–µ—Ü —Å —Ä—ã–Ω–∫–∞ –∏ —Ç.–¥. –ü–µ—Ä–µ–≤–æ–¥–∏ —Å–∞–Ω–∫—Ü–∏–∏, –°–æ–≤–±–µ–∑, –ú–∏–Ω–æ–±–æ—Ä–æ–Ω—ã –Ω–∞ —è–∑—ã–∫ –±—ã—Ç–∞: "—Å–∞–Ω–∫—Ü–∏–∏ = –ø—ã–ª—å —Å –ó–∞–ø–∞–¥–∞", "–°–æ–≤–±–µ–∑ = –¥–∏—Å–ø–µ—Ç—á–µ—Ä—Å–∫–∞—è –ø–æ –±–∞–∑–∞—Ä—É". –ù–∞—á–∏–Ω–∞–π –ø–æ—Å—Ç —Å –∂–∏–≤–æ–π —Å—Ü–µ–Ω—ã (–ª–∞–≤–∫–∞, –æ–≥–æ—Ä–æ–¥, –ø–∏–≤–Ω–æ–π –ª–∞—Ä—ë–∫...), –¥–æ–±–∞–≤–ª—è–π –¥–µ—Ç–∞–ª–∏ –≤—Ä–æ–¥–µ "–≤–æ–¥–∫–∞ –ø–æ–¥–æ—Ä–æ–∂–∞–ª–∞", "—É –º–µ–Ω—è –æ–≥—É—Ä—Ü—ã —Å–æ–ª–∏—Ç—å", "–ü–µ—Ç—Ä–æ–≤–∏—á, –¥—Ä–∞—Ç—å –µ–≥–æ –≤ —Å—Ä–∞–∫—É!". –ó–∞–∫–∞–Ω—á–∏–≤–∞–π –∏—Ä–æ–Ω–∏—á–Ω–æ: "–ê –º–Ω–µ-—Ç–æ —á—ë? –£ –º–µ–Ω—è –≥–∞—Ä–∞–∂ –µ—Å—Ç—å". –ü–æ–¥–ø–∏—Å—å: "–ó–∞ –†–æ–¥–∏–Ω—É-–º–∞—Ç—å –Ω–µ —Å—Ç—ã–¥–Ω–æ —Ä–≤–∞—Ç—å!" üá∑üá∫.

–í–æ—Ç –Ω–æ–≤–æ—Å—Ç—å: "{title}. {summary}"

–ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ –í–∏—Ç—ë–∫–∞. –í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å —Å—Ç—Ä–æ–∫—É:  
–ü–†–û–ú–ü–¢ –î–õ–Ø –ö–ê–†–¢–ò–ù–ö–ò: [–æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã –≤ —Å—Ç–∏–ª–µ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏, —Å —é–º–æ—Ä–æ–º, –∞–±—Å—É—Ä–¥–æ–º, –¥–µ—Ç–∞–ª—è–º–∏, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏].
"""
    prompt = PROMPT_TEMPLATE.format(title=title, summary=summary)

    print("üìù –û—Ç–ø—Ä–∞–≤–ª—è—é –ø—Ä–æ–º–ø—Ç –≤ LLM:")
    print("-" * 50)
    print(prompt[:500] + "..." if len(prompt) > 500 else prompt)
    print("-" * 50)

    try:
        HF_TOKEN = os.environ["HF_TOKEN"]
        MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"

        # URL –¥–ª—è —á–∞—Ç-–º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ HF Inference API
        API_URL = f"https://api-inference.huggingface.co/models/{MODEL_ID}"

        headers = {
            "Authorization": f"Bearer {HF_TOKEN}",
            "Content-Type": "application/json"
        }

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ payload –¥–ª—è —á–∞—Ç-–º–æ–¥–µ–ª–∏
        payload = {
            "inputs": prompt, # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —á–∞—Ç-–º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –æ–∂–∏–¥–∞—Ç—å inputs, –∞ –Ω–µ messages
            "parameters": {
                "temperature": 0.9,
                "max_new_tokens": 600, # –ò—Å–ø–æ–ª—å–∑—É–µ–º max_new_tokens –≤–º–µ—Å—Ç–æ max_tokens
                # "return_full_text": False, # –û–±—ã—á–Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            },
            "options": {
                "wait_for_model": True # –ñ–¥–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ –≤—ã–≥—Ä—É–∂–µ–Ω–∞
            }
        }

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π payload –¥–ª—è API, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç messages (–µ—Å–ª–∏ –ø–µ—Ä–≤—ã–π –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç)
        # payload = {
        #     "messages": [{"role": "user", "content": prompt}],
        #     "parameters": {
        #         "temperature": 0.9,
        #         "max_new_tokens": 600,
        #     },
        #     "options": {
        #         "wait_for_model": True
        #     }
        # }

        response = requests.post(API_URL, headers=headers, json=payload, timeout=60)
        response.raise_for_status() # –í—ã–∑—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å != 200

        result_json = response.json()
        print(f"–û—Ç–≤–µ—Ç –æ—Ç API: {result_json}") # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
        # –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏ –∏ API
        # –û–±—ã—á–Ω–æ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –∏–ª–∏ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å
        if isinstance(result_json, list) and len(result_json) > 0:
            generated_text = result_json[0].get("generated_text", "")
        elif isinstance(result_json, dict):
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∫–ª—é—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è
            generated_text = result_json.get("generated_text", "")
            # –ï—Å–ª–∏ –∫–ª—é—á 'generated_text' –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–º–æ–∂–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç –¥–ª—è chat
            # –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ 'choices' –∏–ª–∏ –Ω–∞–ø—Ä—è–º—É—é –≤ 'message'
            if not generated_text and 'choices' in result_json:
                generated_text = result_json['choices'][0]['message']['content']
        else:
            raise ValueError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç API: {result_json}")

        # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –µ—Å–ª–∏ –æ–Ω –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è
        if generated_text.startswith(prompt):
            generated_text = generated_text[len(prompt):].strip()

        print("‚úÖ LLM –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç:")
        print("-" * 50)
        print(generated_text[:500] + "..." if len(generated_text) > 500 else generated_text)
        print("-" * 50)
        return generated_text

    except requests.exceptions.HTTPError as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê HTTP –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: {e}")
        print(f"–°—Ç–∞—Ç—É—Å –∫–æ–¥: {e.response.status_code}")
        print(f"–¢–µ–ª–æ –æ—Ç–≤–µ—Ç–∞: {e.response.text}")
        raise
    except Exception as e:
        print("‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM:")
        print(f"–¢–∏–ø: {type(e).__name__}")
        print(f"–°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)}")
        print("–ü–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ç–µ–∫:")
        traceback.print_exc()
        raise  # –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º, —á—Ç–æ–±—ã workflow —É–ø–∞–ª ‚Äî —ç—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏


# === KANDINSKY ===
def generate_image_with_kandinsky(prompt):
    url = "https://api.fusionbrain.ai/api/v1/text2image"
    headers = {
        "X-Key": FB_API_KEY,
        "Content-Type": "application/json"
    }
    payload = {
        "model_id": "7582",
        "params": {
            "prompt": prompt + ", russian provincial town, humorous, detailed, no text, no letters",
            "negative_prompt": "blurry, ugly, text, signature, watermark, deformed",
            "width": 1024,
            "height": 1024,
            "steps": 30,
            "seed": random.randint(1, 1000000)
        }
    }

    response = requests.post(url, headers=headers, json=payload, timeout=120)
    if response.status_code != 200:
        raise Exception(f"Kandinsky error: {response.text}")
    
    image_url = response.json()["result"][0]["image_url"]
    img_data = requests.get(image_url).content
    img_path = "/tmp/vitok_post.jpg"
    with open(img_path, "wb") as f:
        f.write(img_data)
    return img_path

# === TELEGRAM ===
def send_to_telegram(text, image_path=None):
    base_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}"
    data = {
        "chat_id": CHANNEL,
        "text": text[:4096],
        "parse_mode": "HTML"
    }
    requests.post(f"{base_url}/sendMessage", data=data)

    if image_path:
        try:
            with open(image_path, "rb") as img:
                files = {"photo": img}
                data = {"chat_id": CHANNEL}
                requests.post(f"{base_url}/sendPhoto", files=files, data=data)
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É: {e}")

# === MAIN ===
if __name__ == "__main__":
    print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Gist...")
    seen_titles = load_seen()

    print("üîç –ò—â—É —Å–≤–µ–∂–∏–µ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏...")
    news = fetch_political_news(hours=1)

    if not news:
        print("üò¥ –ù–µ—Ç —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å.")
        save_seen(seen_titles)
        exit(0)

    item = news[0]
    print(f"üì∞ –ù–∞—à—ë–ª: {item['title']}")

    full_output = ""
    try:
        print("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É—é –ø–æ—Å—Ç —á–µ—Ä–µ–∑ LLM...")
        full_output = generate_post_with_llm(item["title"], item["summary"])

        if "–ü–†–û–ú–ü–¢ –î–õ–Ø –ö–ê–†–¢–ò–ù–ö–ò:" in full_output:
            text_part, img_prompt_raw = full_output.split("–ü–†–û–ú–ü–¢ –î–õ–Ø –ö–ê–†–¢–ò–ù–ö–ò:", 1)
            text = text_part.strip()
            img_prompt = img_prompt_raw.strip().strip("[]\"' ")
        else:
            text = full_output
            img_prompt = "A Russian man on a bench in a small town, reading news, beer bottle nearby, humorous style"

        print("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É...")
        img_path = generate_image_with_kandinsky(img_prompt)

        print("üì§ –ü–æ—Å—Ç–∏–º –≤ Telegram...")
        send_to_telegram(text, img_path)

        print("‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ!")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        fallback_text = f"[‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏]\n\n{full_output[:4000] if full_output else item['title']}"
        send_to_telegram(fallback_text)

    save_seen(seen_titles)

print("üèÅ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π:", len(news))
